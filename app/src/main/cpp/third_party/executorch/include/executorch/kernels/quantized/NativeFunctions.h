/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <executorch/runtime/core/exec_aten/exec_aten.h> // at::Tensor etc.
#include <executorch/runtime/kernel/kernel_runtime_context.h>

// @generated by torchgen/gen_executorch.py from NativeFunctions.h

namespace torch {
namespace executor {
namespace native {
torch::executor::Tensor & quantized_add_out(const torch::executor::Tensor & a, double a_scale, int64_t a_zero_point, int64_t a_quant_min, int64_t a_quant_max, const torch::executor::Tensor & b, double b_scale, int64_t b_zero_point, int64_t b_quant_min, int64_t b_quant_max, double out_scale, int64_t out_zero_point, int64_t out_quant_min, int64_t out_quant_max, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_add_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & a, double a_scale, int64_t a_zero_point, int64_t a_quant_min, int64_t a_quant_max, const torch::executor::Tensor & b, double b_scale, int64_t b_zero_point, int64_t b_quant_min, int64_t b_quant_max, double out_scale, int64_t out_zero_point, int64_t out_quant_min, int64_t out_quant_max, torch::executor::Tensor & out);
::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> choose_qparams_tensor_out(const torch::executor::Tensor & input, int64_t quant_min, int64_t quant_max, double eps, torch::executor::ScalarType dtype, torch::executor::Tensor & scale_out, torch::executor::Tensor & zero_point_out);
::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> choose_qparams_tensor_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, int64_t quant_min, int64_t quant_max, double eps, torch::executor::ScalarType dtype, torch::executor::Tensor & scale_out, torch::executor::Tensor & zero_point_out);
torch::executor::Tensor & dequantize_per_tensor_out(const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_tensor_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_tensor_tensor_args_out(const torch::executor::Tensor & input, const torch::executor::Tensor & scale, const torch::executor::Tensor & zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_tensor_tensor_args_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scale, const torch::executor::Tensor & zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_channel_out(const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_channel_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_channel_out(const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::optional<torch::executor::Tensor> & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_channel_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::optional<torch::executor::Tensor> & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::optional<torch::executor::ScalarType> out_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_byte_out(const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_byte_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_byte_dtype_out(const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_byte_dtype_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_2bit_out(const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_2bit_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_2bit_dtype_out(const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_2bit_dtype_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_4bit_out(const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_4bit_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_4bit_dtype_out(const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_embedding_4bit_dtype_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, int64_t weight_quant_min, int64_t weight_quant_max, const torch::executor::Tensor & indices, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_mixed_mm_out(const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_mixed_mm_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_mixed_linear_out(const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantized_mixed_linear_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & weight, const torch::executor::Tensor & weight_scales, const torch::executor::optional<torch::executor::Tensor> & weight_zero_points, torch::executor::optional<torch::executor::ScalarType> dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_tensor_out(const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_tensor_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_tensor_tensor_args_out(const torch::executor::Tensor & input, const torch::executor::Tensor & scale, const torch::executor::Tensor & zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_tensor_tensor_args_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scale, const torch::executor::Tensor & zero_point, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> choose_qparams_per_token_asymmetric_out(const torch::executor::Tensor & input, torch::executor::ScalarType dtype, torch::executor::Tensor & scale_out, torch::executor::Tensor & zero_point_out);
::std::tuple<torch::executor::Tensor &,torch::executor::Tensor &> choose_qparams_per_token_asymmetric_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, torch::executor::ScalarType dtype, torch::executor::Tensor & scale_out, torch::executor::Tensor & zero_point_out);
torch::executor::Tensor & quantize_per_token_out(const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & quantize_per_token_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_token_out(const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::ScalarType output_dtype, torch::executor::Tensor & out);
torch::executor::Tensor & dequantize_per_token_out(torch::executor::KernelRuntimeContext & context, const torch::executor::Tensor & input, const torch::executor::Tensor & scales, const torch::executor::Tensor & zero_points, int64_t quant_min, int64_t quant_max, torch::executor::ScalarType dtype, torch::executor::ScalarType output_dtype, torch::executor::Tensor & out);
} // namespace native
} // namespace executor
} // namespace torch
